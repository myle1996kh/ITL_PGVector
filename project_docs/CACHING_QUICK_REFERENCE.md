# Caching Quick Reference

## TL;DR - Answer to Your Questions

### Q1: "What is the checkpoints table for?"
**A:** LangGraph checkpoints store conversation **state history** so you can resume long conversations. Currently not heavily used in basic chat, but critical for **multi-turn agent workflows**.

### Q2: "Whenever I send request - only configs Agent & tool one at the first time right?"
**A:** **YES, EXACTLY RIGHT!** Here's what gets cached:

```
Request 1 (FIRST):
â”œâ”€ Agent Config: âŒ LOAD FROM DB (no cache)
â”œâ”€ LLM: âŒ LOAD & CACHE (first time)
â”œâ”€ Tools: âŒ LOAD & CACHE (first time)
â””â”€ Total: ~300-500ms CACHE LOAD TIME

Request 2 (SECOND, SAME TENANT):
â”œâ”€ Agent Config: âŒ LOAD FROM DB (always fresh)
â”œâ”€ LLM: âœ… USE CACHE (300ms saved!)
â”œâ”€ Tools: âœ… USE CACHE (500ms saved!)
â””â”€ Total: ~800ms saved! ğŸ‰
```

---

## What Gets Cached?

### âœ… CACHED (Reused):
- **LLM Clients** (ChatOpenAI instance)
  - Cache key: `llm:{tenant_id}:{model_id}`
  - Duration: Server lifetime
  - Saves: ~300ms per request

- **Tools** (StructuredTool instances)
  - Cache key: `{tenant_id}:{tool_id}`
  - Duration: Server lifetime
  - Saves: ~50-100ms per tool

### âŒ NOT CACHED (Always fresh):
- **Agent Config** (agent prompt, agent settings)
  - Why: Must be up-to-date if admin changes settings
  - Time: ~20-50ms to query

- **Tool Calls** (which tools to invoke)
  - Why: Depends on user message & LLM decision
  - Time: ~2000ms (LLM inference)

- **API Responses** (HTTP tool results)
  - Why: Data changes constantly
  - Time: ~500-1000ms (external API)

---

## When Cache is Populated

```
INITIALIZATION (Server starts):
  LLM Cache: Empty
  Tool Cache: Empty

FIRST REQUEST (User sends message):
  [Load LLM]
    â”œâ”€ Check cache? No
    â”œâ”€ Query database for tenant_llm_config
    â”œâ”€ Query database for llm_model
    â”œâ”€ Create ChatOpenAI() instance
    â””â”€ SAVE TO CACHE â† LLM Cache now has entry

  [Load Tools]
    â”œâ”€ For each tool:
    â”‚  â”œâ”€ Check cache? No
    â”‚  â”œâ”€ Query database for tool_config
    â”‚  â”œâ”€ Create StructuredTool() instance
    â”‚  â””â”€ SAVE TO CACHE â† Tool Cache now has entry

SECOND REQUEST (Same tenant, same agent):
  [Load LLM]
    â”œâ”€ Check cache? YES âœ“
    â”œâ”€ Return cached ChatOpenAI instance
    â””â”€ Skip all DB queries!

  [Load Tools]
    â”œâ”€ For each tool:
    â”‚  â”œâ”€ Check cache? YES âœ“
    â”‚  â””â”€ Return cached StructuredTool instance

THIRD REQUEST (Different tenant):
  [Load LLM]
    â”œâ”€ Check cache? No (different tenant_id)
    â”œâ”€ Query database (new tenant config)
    â””â”€ Create & SAVE new LLM instance

  [Load Tools]
    â”œâ”€ For each tool:
    â”‚  â”œâ”€ Check cache? No (different tenant_id)
    â”‚  â””â”€ Create & SAVE new tool instance
```

---

## Cache Key Patterns

### LLM Cache Keys:
```
llm:{tenant_id}:{llm_model_id}

Examples:
â”œâ”€ llm:2628802d-1dff-4a98-9325-704433c5d3ab:b43d2ad7-0ddb-4a56-b37d-e42c6e3070e8
â”œâ”€ llm:2628802d-1dff-4a98-9325-704433c5d3ab:default
â””â”€ llm:other-tenant-id:other-model-id
```

### Tool Cache Keys:
```
{tenant_id}:{tool_id}

Examples:
â”œâ”€ 2628802d-1dff-4a98-9325-704433c5d3ab:tool-id-1
â”œâ”€ 2628802d-1dff-4a98-9325-704433c5d3ab:tool-id-2
â””â”€ other-tenant-id:tool-id-3
```

---

## How to Clear Cache (When Needed)

### Clear LLM Cache:
```python
from src.services.llm_manager import llm_manager

# Clear specific tenant
llm_manager.clear_cache(tenant_id="2628802d-...")

# Clear all
llm_manager.clear_cache()
```

### Clear Tool Cache:
```python
from src.services.tool_loader import tool_registry

# Clear specific tenant
tool_registry.clear_cache(tenant_id="2628802d-...")

# Clear all
tool_registry.clear_cache()
```

### When to clear:
- âœ… After updating LLM config (API key change, model change)
- âœ… After updating tool config (endpoint URL change)
- âœ… When deploying new code
- âœ… For debugging cache issues

---

## Monitoring Cache Performance

### Check logs for cache hits:
```
âœ… llm_cache_hit    - LLM was cached, saved ~300ms
âœ… tool_cache_hit   - Tool was cached, saved ~50ms
âŒ llm_client_created - LLM not cached, took ~300ms
âŒ tool_created     - Tool not cached, took ~100ms
```

### Benchmarks:
```
WITH CACHE (Requests 2+):
â”œâ”€ Load LLM: 1-5ms âœ“
â”œâ”€ Load 5 Tools: 25-50ms âœ“
â””â”€ Total config load: 30-60ms

WITHOUT CACHE (Request 1):
â”œâ”€ Load LLM: 300-500ms
â”œâ”€ Load 5 Tools: 500ms (100ms Ã— 5)
â””â”€ Total config load: 800-1000ms

SAVED PER REQUEST: ~750-950ms! ğŸš€
```

---

## LangGraph Checkpoints (For Future)

Currently not heavily used, but important to know:

### Purpose:
- Resume interrupted conversations
- Complex multi-step agent workflows
- Conversation history & context

### Tables:
- `langgraph_checkpoints` - Stores state snapshots
- `langgraph_writes` - Stores state transitions
- `langgraph_migrations` - Schema versioning

### When used:
- Multi-turn conversations with memory
- Agent loops (agent â†’ tool â†’ agent â†’ tool...)
- Complex reasoning workflows

---

## Summary Table

| Component | Cached? | Cache Key | When Populated | When Cleared | Time Saved |
|-----------|---------|-----------|-----------------|--------------|-----------|
| **Agent Config** | âŒ No | N/A | Always queried | N/A | 0ms |
| **LLM Client** | âœ… Yes | `llm:{tenant}:{model}` | First request | Manual/Deploy | 300-500ms |
| **Tools** | âœ… Yes | `{tenant}:{tool_id}` | First request | Manual/Deploy | 50ms each |
| **Checkpoints** | âœ… Yes (DB) | `thread_id` | Multi-turn | Auto | Enables resuming |
| **Tool Results** | âŒ No | N/A | Every request | N/A | 0ms |
| **API Responses** | âŒ No | N/A | Every request | N/A | 0ms |

---

## Code Locations

- **LLM Manager Cache**: `src/services/llm_manager.py:16-89`
- **Tool Registry Cache**: `src/services/tool_loader.py:30-147`
- **Checkpoints Service**: `src/services/checkpoint_service.py:11-107`

