{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pgvector RAG Implementation Test\n",
    "\n",
    "**Purpose**: Test pgvector-based RAG before migrating from ChromaDB\n",
    "\n",
    "**Input Data**: eTMS USER GUIDE DOCUMENT.pdf\n",
    "\n",
    "**Testing Plan**:\n",
    "1. Setup PostgreSQL connection with pgvector\n",
    "2. Extract text from PDF\n",
    "3. Chunk documents intelligently\n",
    "4. Generate embeddings using sentence-transformers\n",
    "5. Store vectors in PostgreSQL\n",
    "6. Create HNSW index for fast similarity search\n",
    "7. Test retrieval quality\n",
    "8. Benchmark performance\n",
    "9. Compare with ChromaDB approach\n",
    "\n",
    "**Date**: 2025-11-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install psycopg2-binary sentence-transformers pypdf2 numpy pandas tqdm langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to PostgreSQL\n",
      "PostgreSQL Version: PostgreSQL 18.0 on x86_64-windows, compiled by msv...\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import os\n",
    "\n",
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'chatbot_db',  # Update if different\n",
    "    'user': 'postgres',\n",
    "    'password': os.getenv('DB_PASSWORD', '123456')  # Use environment variable\n",
    "}\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    print(\"‚úÖ Connected to PostgreSQL\")\n",
    "    \n",
    "    # Check PostgreSQL version\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT version();\")\n",
    "        version = cur.fetchone()[0]\n",
    "        print(f\"PostgreSQL Version: {version[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enable pgvector Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå pgvector setup failed: extension \"vector\" is not available\n",
      "HINT:  The extension must first be installed on the system where PostgreSQL is running.\n",
      "\n",
      "Install pgvector: https://github.com/pgvector/pgvector#installation\n"
     ]
    }
   ],
   "source": [
    "# Enable pgvector extension\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "        print(\"‚úÖ pgvector extension enabled\")\n",
    "        \n",
    "        # Verify extension\n",
    "        cur.execute(\"SELECT extname, extversion FROM pg_extension WHERE extname = 'vector';\")\n",
    "        result = cur.fetchone()\n",
    "        if result:\n",
    "            print(f\"pgvector version: {result[1]}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è pgvector not found - may need manual installation\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå pgvector setup failed: {e}\")\n",
    "    print(\"Install pgvector: https://github.com/pgvector/pgvector#installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Document Table with Vector Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped existing table (clean slate)\n",
      "‚ùå Table creation failed: type \"vector\" does not exist\n",
      "LINE 8:     embedding vector(384),  -- 384 dimensions for all-MiniLM...\n",
      "                      ^\n",
      "\n"
     ]
    },
    {
     "ename": "UndefinedObject",
     "evalue": "type \"vector\" does not exist\nLINE 8:     embedding vector(384),  -- 384 dimensions for all-MiniLM...\n                      ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedObject\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müóëÔ∏è Dropped existing table (clean slate)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m         \u001b[38;5;66;03m# Create new table\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCREATE_TABLE_SQL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Created document_chunks table with vector column (384 dimensions)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mUndefinedObject\u001b[39m: type \"vector\" does not exist\nLINE 8:     embedding vector(384),  -- 384 dimensions for all-MiniLM...\n                      ^\n"
     ]
    }
   ],
   "source": [
    "# Create table for document chunks\n",
    "# Using 384 dimensions for all-MiniLM-L6-v2 model\n",
    "\n",
    "CREATE_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS document_chunks (\n",
    "    chunk_id SERIAL PRIMARY KEY,\n",
    "    document_name VARCHAR(255) NOT NULL,\n",
    "    page_number INTEGER,\n",
    "    chunk_text TEXT NOT NULL,\n",
    "    chunk_index INTEGER NOT NULL,\n",
    "    embedding vector(384),  -- 384 dimensions for all-MiniLM-L6-v2\n",
    "    metadata JSONB,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        # Drop existing table for clean test\n",
    "        cur.execute(\"DROP TABLE IF EXISTS document_chunks;\")\n",
    "        print(\"üóëÔ∏è Dropped existing table (clean slate)\")\n",
    "        \n",
    "        # Create new table\n",
    "        cur.execute(CREATE_TABLE_SQL)\n",
    "        print(\"‚úÖ Created document_chunks table with vector column (384 dimensions)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Table creation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from typing import List, Dict\n",
    "\n",
    "PDF_PATH = \"eTMS USER GUIDE DOCUMENT.pdf\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, any]]:\n",
    "    \"\"\"Extract text from PDF page by page.\"\"\"\n",
    "    pages = []\n",
    "    \n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            total_pages = len(pdf_reader.pages)\n",
    "            \n",
    "            print(f\"üìÑ Processing {total_pages} pages from {pdf_path}\")\n",
    "            \n",
    "            for page_num in range(total_pages):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "                \n",
    "                if text.strip():\n",
    "                    pages.append({\n",
    "                        'page_number': page_num + 1,\n",
    "                        'text': text.strip()\n",
    "                    })\n",
    "            \n",
    "            print(f\"‚úÖ Extracted {len(pages)} pages with content\")\n",
    "            return pages\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå PDF not found: {pdf_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PDF extraction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Extract text\n",
    "pages = extract_text_from_pdf(PDF_PATH)\n",
    "\n",
    "# Display sample\n",
    "if pages:\n",
    "    print(\"\\nüìÑ Sample from first page:\")\n",
    "    print(pages[0]['text'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chunk Documents & Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk.strip())\n",
    "        start += chunk_size - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "def chunk_pages(pages: List[Dict], chunk_size: int = 500) -> List[Dict]:\n",
    "    \"\"\"Chunk all pages with metadata.\"\"\"\n",
    "    all_chunks = []\n",
    "    for page in pages:\n",
    "        page_chunks = chunk_text(page['text'], chunk_size)\n",
    "        for idx, chunk in enumerate(page_chunks):\n",
    "            all_chunks.append({\n",
    "                'page_number': page['page_number'],\n",
    "                'chunk_index': idx,\n",
    "                'text': chunk,\n",
    "                'metadata': {'page': page['page_number'], 'chunk': idx}\n",
    "            })\n",
    "    return all_chunks\n",
    "\n",
    "# Chunk documents\n",
    "chunks = chunk_pages(pages)\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks from {len(pages)} pages\")\n",
    "\n",
    "# Load embedding model\n",
    "print(\"\\nüì• Loading embedding model (all-MiniLM-L6-v2)...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Model loaded\")\n",
    "\n",
    "# Generate embeddings\n",
    "print(f\"\\nüî¢ Generating embeddings for {len(chunks)} chunks...\")\n",
    "chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "print(f\"‚úÖ Generated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Store Vectors in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "DOCUMENT_NAME = \"eTMS USER GUIDE DOCUMENT.pdf\"\n",
    "\n",
    "INSERT_SQL = \"\"\"\n",
    "INSERT INTO document_chunks (\n",
    "    document_name, page_number, chunk_text, chunk_index, embedding, metadata\n",
    ") VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cur:\n",
    "        print(f\"üíæ Storing {len(chunks)} chunks in database...\")\n",
    "        \n",
    "        for chunk, embedding in tqdm(zip(chunks, embeddings), total=len(chunks)):\n",
    "            cur.execute(INSERT_SQL, (\n",
    "                DOCUMENT_NAME,\n",
    "                chunk['page_number'],\n",
    "                chunk['text'],\n",
    "                chunk['chunk_index'],\n",
    "                embedding.tolist(),\n",
    "                json.dumps(chunk['metadata'])\n",
    "            ))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"‚úÖ All chunks stored successfully\")\n",
    "        \n",
    "        # Verify count\n",
    "        cur.execute(\"SELECT COUNT(*) FROM document_chunks;\")\n",
    "        count = cur.fetchone()[0]\n",
    "        print(f\"üìä Total chunks in database: {count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    conn.rollback()\n",
    "    print(f\"‚ùå Storage failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create HNSW Index & Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create HNSW index\n",
    "CREATE_INDEX_SQL = \"\"\"\n",
    "CREATE INDEX IF NOT EXISTS document_chunks_embedding_idx\n",
    "ON document_chunks\n",
    "USING hnsw (embedding vector_cosine_ops)\n",
    "WITH (m = 16, ef_construction = 64);\n",
    "\"\"\"\n",
    "\n",
    "print(\"üî® Creating HNSW index...\")\n",
    "start_time = time.time()\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(CREATE_INDEX_SQL)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úÖ HNSW index created in {elapsed:.2f} seconds\")\n",
    "\n",
    "# Test search function\n",
    "def search_similar_chunks(conn, query: str, embedding_model, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Search for similar document chunks.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query])[0].tolist()\n",
    "    \n",
    "    SEARCH_SQL = \"\"\"\n",
    "    SELECT chunk_id, document_name, page_number, chunk_text, chunk_index, metadata,\n",
    "           1 - (embedding <=> %s::vector) AS similarity_score\n",
    "    FROM document_chunks\n",
    "    ORDER BY embedding <=> %s::vector\n",
    "    LIMIT %s;\n",
    "    \"\"\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(SEARCH_SQL, (query_embedding, query_embedding, top_k))\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        return [{\n",
    "            'chunk_id': row[0],\n",
    "            'document_name': row[1],\n",
    "            'page_number': row[2],\n",
    "            'text': row[3],\n",
    "            'chunk_index': row[4],\n",
    "            'metadata': row[5],\n",
    "            'similarity_score': float(row[6])\n",
    "        } for row in results]\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How do I track shipments?\",\n",
    "    \"What is the login process?\",\n",
    "    \"How to generate reports?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing similarity search...\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = search_similar_chunks(conn, query, embedding_model, top_k=3)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Search time: {search_time*1000:.2f}ms\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"Result {i} (Score: {result['similarity_score']:.4f})\")\n",
    "        print(f\"Page {result['page_number']}, Chunk {result['chunk_index']}\")\n",
    "        print(f\"Text: {result['text'][:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä pgvector RAG Test Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT COUNT(*) FROM document_chunks;\")\n",
    "    total_chunks = cur.fetchone()[0]\n",
    "    \n",
    "    cur.execute(\"SELECT COUNT(DISTINCT page_number) FROM document_chunks;\")\n",
    "    total_pages = cur.fetchone()[0]\n",
    "    \n",
    "    print(f\"\\nüìÑ Document Statistics:\")\n",
    "    print(f\"  - Total Pages: {total_pages}\")\n",
    "    print(f\"  - Total Chunks: {total_chunks}\")\n",
    "    print(f\"  - Avg Chunks/Page: {total_chunks/total_pages:.1f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Status: pgvector RAG test successful!\")\n",
    "    print(f\"\\nüìã Next Steps:\")\n",
    "    print(f\"  1. Review test results and performance\")\n",
    "    print(f\"  2. Follow full migration plan in PGVECTOR_MIGRATION_PLAN.md\")\n",
    "    print(f\"  3. Migrate existing ChromaDB data to pgvector\")\n",
    "    print(f\"  4. Update chatbot agents to use pgvector\")\n",
    "    print(f\"  5. Test in production environment\")\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
